from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate,
    MessagesPlaceholder
)

import streamlit as st
from streamlit_chat import message  # a Python package that provides a chatbot interface for Streamlit applications
from utils import *

st.title("–ú–û–ù–ï–¢–ê Chatbot :books:") # giving a cool name for our Chatbot using st.title() function
st.markdown(" Powered by ü¶ú LangChain + üß† OpenAI + üöÄ Pinecone + üí¨ Streamlit")

# initialize the chatbot by giving it a starter message at the first app run:
if 'responses' not in st.session_state:
    st.session_state['responses'] = ["–ö–∞–∫ –º–æ–≥–∞ –¥–∞ –í–∏ –ø–æ–º–æ–≥–Ω–∞?"]

if 'requests' not in st.session_state:
    st.session_state['requests'] = []

llm = ChatOpenAI(model_name="gpt-3.5-turbo",
                 openai_api_key="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")

# creating a ConversationBufferWindowMemory object with k=3 and assigns it to the session state variable 'buffer_memory' 
# ConversationBufferWindowMemory keeps the latest pieces of the conversation in raw form

if 'buffer_memory' not in st.session_state:
    st.session_state.buffer_memory = ConversationBufferWindowMemory(k=3, return_messages=True)
    # chat_memory=ChatMessageHistory(messages=[]) output_key=None input_key=None return_messages=True human_prefix='Human' ai_prefix='AI' memory_key='history' k=3

system_msg_template = SystemMessagePromptTemplate.from_template(
    template="""–í–∏–µ —Å—Ç–µ –ú–û–ù–ï–¢–ê –æ—Ç–≥–æ–≤–æ—Ä–Ω–∏–∫, –ø–æ–∑–Ω–∞–≤–∞—Ç–µ –ø–µ—Ä—Ñ–µ–∫—Ç–Ω–æ –ø—Ä–æ–≥—Ä–∞–º–∞—Ç–∞ –ú–û–ù–ï–¢–ê, —Ä–∞–∑–±–∏—Ä–∞—Ç–µ —è, –∏ –∏–º–∞—Ç–µ –æ–ø–∏—Ç —Å –Ω–µ—è –ø–æ–≤–µ—á–µ –æ—Ç 10 –≥–æ–¥–∏–Ω–∏. 
    –í–∏–µ —Å—Ç–µ –≤—ä–∑–ø–∏—Ç–∞–Ω, –¥–æ–±—Ä–æ–Ω–∞–º–µ—Ä–µ–Ω, –ø—Ä–∏—è—Ç–µ–ª—Å–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∫—ä–º –≤—Å–∏—á–∫–∏ —Ö–æ—Ä–∞ –∏ –º–Ω–æ–≥–æ —É—á—Ç–∏–≤. 
    –ú–æ–∂–µ—Ç–µ –¥–∞ –ø—Ä–∞–≤–∏—Ç–µ –≤—Å–∏—á–∫–∏ –∑–∞–¥–∞—á–∏ –∏ –¥–∞ –æ—Ç–≥–æ–≤–∞—Ä—è—Ç–µ –Ω–∞ –≤—Å—è–∫–∞–∫–≤–∏ –≤—ä–ø—Ä–æ—Å–∏ —Å–≤—ä—Ä–∑–∞–Ω–∏ —Å –ú–û–ù–ï–¢–ê.
    –û—Ç–≥–æ–≤–æ—Ä–µ—Ç–µ –Ω–∞ –≤—ä–ø—Ä–æ—Å–∞ –≤—ä–∑–º–æ–∂–Ω–æ –Ω–∞–π-–≤—è—Ä–Ω–æ, –∫–∞—Ç–æ –∏–∑–ø–æ–ª–∑–≤–∞—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç,
    –∏ –∞–∫–æ –æ—Ç–≥–æ–≤–æ—Ä—ä—Ç –Ω–µ —Å–µ —Å—ä–¥—ä—Ä–∂–∞ –≤ —Ç–µ–∫—Å—Ç–∞ –ø–æ-–¥–æ–ª—É, –∫–∞–∂–µ—Ç–µ '–í—ä–ø—Ä–æ—Å—ä—Ç –Ω–µ –µ —Å–≤—ä—Ä–∑–∞–Ω —Å –ø—Ä–æ–≥—Ä–∞–º–∞—Ç–∞.'"""
)
human_msg_template = HumanMessagePromptTemplate.from_template(template="{input}")

prompt_template = ChatPromptTemplate.from_messages([system_msg_template, MessagesPlaceholder(variable_name="history"), human_msg_template])

conversation = ConversationChain(memory=st.session_state.buffer_memory, prompt=prompt_template, llm=llm, verbose=True)

# container for chat history
response_container = st.container()
# container for text box
input_container = st.container()
clear_text = st.container()

with input_container:
    # creating a text area for the user input query using st.text_input() function
    query = st.text_input("–í—ä–ø—Ä–æ—Å: ", key="input", placeholder="–ú–û–ù–ï–¢–ê –æ—Ç–≥–æ–≤–∞—Ä—è! –ü–∏—Ç–∞–π—Ç–µ –º–µ ...") 
    if query:
        with st.spinner("–ú–∏—Å–ª—è..."):
            conversation_string = get_conversation_string()
            # st.code(conversation_string)
            refined_query = query_refiner(conversation_string, query)
            # st.subheader("Refined Query:")
            # st.write(refined_query)
            context = find_match(refined_query)
            # # print(context)
             
            response = conversation.predict(input=f"Context:\n {context} \n\n Query:\n{query}")

        st.session_state.requests.append(query)
        st.session_state.responses.append(response)

with clear_text:
    def clear_text():
        st.session_state["input"] = ""
    
    st.button("Clear Text", on_click=clear_text)


# Displaying the response. If there are any generated responses in the st.session_state object, a for loop is initiated. 
# For each generated response, the message() function is called twice to display the query made by the user and the response generated by the llm
# The key parameter is used to uniquely identify each message.
with response_container:
    if st.session_state['responses']: # ['–ö–∞–∫ –º–æ–≥–∞ –¥–∞ –í–∏ –ø–æ–º–æ–≥–Ω–∞?']

        for i in range(len(st.session_state['responses'])):
            message(st.session_state['responses'][i], key=str(i), avatar_style='personas')
            if i < len(st.session_state['requests']):
                message(st.session_state["requests"][i], is_user=True, key=str(i) + '_user')
                

  