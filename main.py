from langchain.chat_models import ChatOpenAI
from dotenv.main import load_dotenv
import os
import streamlit as st
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate,
    MessagesPlaceholder
)
load_dotenv()
openai_key = os.environ['OPENAI_API_KEY']

import streamlit as st
from streamlit_chat import message  # a Python package that provides a chatbot interface for Streamlit applications
from utils import *

load_dotenv()
openai_key = os.environ['OPENAI_API_KEY']
# headers = {
#     "authorization": st.secrets["OPENAI_API_KEY", "PINECONE_API_KEY"],
#     "content-type": "application/json"
# }

st.title("–ú–û–ù–ï–¢–ê Chatbot :books:") # giving a cool name for our Chatbot using st.title() function
st.markdown(" Powered by ü¶ú LangChain + üß† OpenAI + üöÄ Pinecone + üí¨ Streamlit")

# initialize the chatbot by giving it a starter message at the first app run:
if 'responses' not in st.session_state:
    st.session_state['responses'] = ["–ö–∞–∫ –º–æ–≥–∞ –¥–∞ –í–∏ –ø–æ–º–æ–≥–Ω–∞?"]

if 'requests' not in st.session_state:
    st.session_state['requests'] = []

llm = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=openai_key)

# creating a ConversationBufferWindowMemory object with k=3 and assigns it to the session state variable 'buffer_memory' 
# ConversationBufferWindowMemory keeps the latest pieces of the conversation in raw form

if 'buffer_memory' not in st.session_state:
    st.session_state.buffer_memory = ConversationBufferWindowMemory(k=3, return_messages=True)
    # chat_memory=ChatMessageHistory(messages=[]) output_key=None input_key=None return_messages=True human_prefix='Human' ai_prefix='AI' memory_key='history' k=3

system_msg_template = SystemMessagePromptTemplate.from_template(
    template="""
    –¢–∏ —Å–∏ –æ–ø–∏—Ç–µ–Ω —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –≤ –ø–æ–¥–¥—Ä—ä–∂–∫–∞ –Ω–∞ ERP —Å–∏—Å—Ç–µ–º–∞ –ú–æ–Ω–µ—Ç–∞. 
    –¢–≤–æ—è—Ç–∞ –∑–∞–¥–∞—á–∞ –µ –¥–∞ –ø–æ–º–∞–≥–∞—à –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–∏—Ç–µ —Å –µ–∂–µ–¥–Ω–µ–≤–Ω–∞—Ç–∞ –∏–º —Ä–∞–±–æ—Ç–∞, –∫–∞—Ç–æ –æ—Ç–≥–æ–≤–∞—Ä—è—à –Ω–∞ –≤—ä–ø—Ä–æ—Å–∏ –æ—Ç —Ä–∞–ª–∏—á–Ω–æ –µ—Å—Ç–µ—Å—Ç–≤–æ, 
    —Å–≤—ä—Ä–∑–∞–Ω–∏ —Å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç–∏ –≤ –ú–æ–Ω–µ—Ç–∞. 
    –ê–∫–æ –Ω–µ —Ä–∞–∑–±–∏—Ä–∞—à –ø—Ä–∞–≤–∏–ª–Ω–æ –≤—ä–ø—Ä–æ—Å–∞, –∫–æ–π—Ç–æ –∑–∞–¥–∞–≤–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è, –º–æ–ª—è –ø–æ–∏—Å–∫–∞–π –¥–æ–ø—ä–ª–Ω–∏—Ç–µ–ª–Ω–∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è, –∑–∞ –¥–∞ —Å–∏ —Å–∏–≥—É—Ä–µ–Ω,
    —á–µ –æ—Ç–≥–æ–≤–æ—Ä–∏—Ç–µ —Å–∞ –º–∞–∫—Å–∏–º–∞–ª–Ω–æ —Ç–æ—á–Ω–∏. 
    –¢–∏ —Ç—Ä—è–±–≤–∞ –¥–∞ –æ—Ç–≥–æ–≤–æ—Ä–∏—à –Ω–∞ –≤—ä–ø—Ä–æ—Å–∞, –∑–∞–ø–∏—Ç–≤–∞–Ω–µ—Ç–æ –∏–ª–∏ –æ–ø–ª–∞–∫–≤–∞–Ω–µ—Ç–æ –≤—ä–∑–º–æ–∂–Ω–æ –Ω–∞–π-–≤—è—Ä–Ω–æ, –∏ –¥–∞ –æ–±—è—Å–Ω–∏—à –≤—Å–∏—á–∫–æ —Å—Ç—ä–ø–∫–∞ –ø–æ —Å—Ç—ä–ø–∫–∞
    –∫–∞—Ç–æ –Ω–∞ 8 –≥–æ–¥–∏—à–Ω–æ –¥–µ—Ç–µ, –∏–∑–ø–æ–ª–∑–≤–∞–π–∫–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–µ–Ω–∏—è —Ç–µ–∫—Å—Ç.
    A–∫–æ –æ—Ç–≥–æ–≤–æ—Ä—ä—Ç –Ω–µ —Å–µ —Å—ä–¥—ä—Ä–∂–∞ –≤ —Ç–æ–∑–∏ —Ç–µ–∫—Å—Ç, –∫–∞–∂–∏ '–í—ä–ø—Ä–æ—Å—ä—Ç –Ω–µ –µ —Å–≤—ä—Ä–∑–∞–Ω —Å –ø—Ä–æ–≥—Ä–∞–º–∞—Ç–∞.'"""
)
human_msg_template = HumanMessagePromptTemplate.from_template(template="{input}")

prompt_template = ChatPromptTemplate.from_messages([system_msg_template, MessagesPlaceholder(variable_name="history"), human_msg_template])

conversation = ConversationChain(memory=st.session_state.buffer_memory, prompt=prompt_template, llm=llm, verbose=True)

# container for chat history
response_container = st.container()
# container for text box
input_container = st.container()
clear_text = st.container()

with input_container:
    # creating a text area for the user input query using st.text_input() function
    query = st.text_input("–í—ä–ø—Ä–æ—Å: ", key="input", placeholder="–ú–û–ù–ï–¢–ê –æ—Ç–≥–æ–≤–∞—Ä—è! –ü–∏—Ç–∞–π—Ç–µ –º–µ ...") 
    if query:
        with st.spinner("–ú–∏—Å–ª—è..."):
            conversation_string = get_conversation_string()
            # st.code(conversation_string)
            refined_query = query_refiner(conversation_string, query)
            # st.subheader("Refined Query:")
            # st.write(refined_query)
            context = find_match(refined_query)
            # # print(context)
             
            response = conversation.predict(input=f"Context:\n {context} \n\n Query:\n{query}")

        st.session_state.requests.append(query)
        st.session_state.responses.append(response)

with clear_text:
    def clear_text():
        st.session_state["input"] = ""
    
    st.button("Clear Text", on_click=clear_text)


# Displaying the response. If there are any generated responses in the st.session_state object, a for loop is initiated. 
# For each generated response, the message() function is called twice to display the query made by the user and the response generated by the llm
# The key parameter is used to uniquely identify each message.
with response_container:
    if st.session_state['responses']: # ['–ö–∞–∫ –º–æ–≥–∞ –¥–∞ –í–∏ –ø–æ–º–æ–≥–Ω–∞?']

        for i in range(len(st.session_state['responses'])):
            message(st.session_state['responses'][i], key=str(i), avatar_style='personas')
            if i < len(st.session_state['requests']):
                message(st.session_state["requests"][i], is_user=True, key=str(i) + '_user')
                

  
                

                

  
